{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reasoning and Acting (ReAct)â€‹\n",
    "\n",
    "Agent to implement ReAct logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__632ae1a420dc4b46b2894dd749f33b2e\"\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://js-playground-01.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"43d936898d7645c3b3c1ebf294f04cd3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [TavilySearchResults(max_results=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the LLM to use\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt4-32k\",\n",
    ")\n",
    "\n",
    "# Construct the ReAct agent\n",
    "agent = create_react_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find information about LangChain.\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"what is LangChain\"\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.langchain.com/', 'content': 'Join Discord\\nContribute on GitHub\\n5.3M\\nMonthly Downloads\\n67k\\nGithub Stars\\n1800+\\nContributors\\n34k+\\nApps Powered\\nReady to build?\\nGo To Docs\\nGo To Docs\\nGo To Docs\\nContact Sales\\nContact Sales\\nContact Sales\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter\\nProducts and use-cases\\nLangChain\\nLangSmith\\nRetrieval\\nAgents\\nInspiration\\nCode\\nGitHub\\nLangChain Hub\\nPython Docs\\nJS/TS Docs\\nSocial\\nTwitter\\nDiscord\\nBlog\\nLinkedIn\\nYouTube\\nTerms of Service\\nSign up for our newsletter Chatbots\\nQ&A Over Docs\\nSummarization\\nCopilots\\nWorkflow Automation\\nDocument Analysis\\nCustom Search\\nHarness the powerâ€“and wrangle the complexityâ€“ of LLMs\\nGet from proof-of-concept to production on one platform\\nRely on a set of tools built especially for non-deterministic models\\nLeverage new cognitive architectures and battle-tested orchestration\\nThanks to our contributors and partners for making LangChain awesome\\nWhen you build with LangChain, you build alongside a community of developers advancing the frontier of whatâ€™s possible with LLMs.\\n ðŸ¦œðŸ”— LangChain\\nLangSmith\\nLangServe\\nAgents\\nRetrieval\\nEvaluation\\nBlog\\nDocs\\nðŸ¦œðŸ”— LangChain\\nðŸ¦œðŸ”— LangChain\\nLangChain Expression Language makes it easy to create custom chains.\\n Go To Docs\\nGo To Docs\\nGo To Docs\\nContact Sales\\nglobal corporations, STARTUPS, and TINKERERS build with LangChain\\nglobal corporations, STARTUPS, and TINKERERS build with LangChain\\nglobal corporations, STARTUPS, and TINKERERS build with LangChain\\nA complete set of powerful building blocks\\nA complete set of powerful building blocks\\n Try it out here\\nGet your LLM application from prototype to production\\nGet your LLM application from prototype to production\\nBuild context-aware, reasoning applications with LangChainâ€™s flexible abstractions and AI-first toolkit.\\n'}]\u001b[0m\u001b[32;1m\u001b[1;3mThe observation does not provide a clear definition of what LangChain is. It mentions that it has a complete set of powerful building blocks and that it can build context-aware, reasoning applications with flexible abstractions and an AI-first toolkit. This information is a bit vague, so I need to search again for a more straightforward definition.\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"LangChain definition\"\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://www.datastax.com/guides/what-is-langchain', 'content': 'See how Datastax and Langchain work better together\\nTo get started with LangChain and to see how DataStax and LangChain are better together check out our recent blog post on Build LLM-Powered Applications Faster with LangChain Templates and Astra DB\\nAlso, you can find more information on how to set up and deploy Astra DB on one of the worldâ€™s highest performing vector stores, built on Apache Cassandra, which is designed for handling massive volumes of data at scale. An embedding is created from that text, a lookup is done on a vector database to gain more context about the question, then a prompt is created using both the original question and the context from the retrieval. The same goes for a chat with the LLM.\\nLangChain offers a special memory library where the application can save a chatâ€™s history to a fast lookup store. That means, that while it has a lot of pre-trained knowledge, it can take in data as itâ€™s submitted to the application and give the LLM up-to-date information.\\n In addition, you might want to generate vector embeddings when your app is following a retrieval augmented generation (RAG) pattern, submit a prompt for the LLM to complete, or open a chat session with the model.'}]\u001b[0m\u001b[32;1m\u001b[1;3mThe observation indicates that LangChain is related to building LLM (Large Language Models)-powered applications and offers a special memory library where the application can save a chatâ€™s history to a fast lookup store. It seems to be capable of processing and understanding data as itâ€™s submitted to the application, providing the LLM with up-to-date information. However, a clear and concise definition is still missing.\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"LangChain purpose\"\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://python.langchain.com/docs/get_started/introduction', 'content': \"Modules\\u200b\\nLangChain provides standard, extendable interfaces and integrations for the following modules:\\nInterface with language models\\nInterface with application-specific data\\nLet models choose which tools to use given high-level directives\\nExamples, ecosystem, and resources\\u200b\\nUse cases\\u200b\\nWalkthroughs and techniques for common end-to-end use cases, like:\\nIntegrations\\u200b\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Together, these products simplify the entire application lifecycle:\\nLangChain Libraries\\u200b\\nThe main value props of the LangChain packages are:\\nOff-the-shelf chains make it easy to get started. API reference\\u200b\\nHead to the reference section for full documentation of all classes and methods in the LangChain and LangChain Experimental Python packages.\\n Developer's guide\\u200b\\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\\n LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest â€œprompt + LLMâ€ chain to the most complex chains.\\n\"}]\u001b[0m\u001b[32;1m\u001b[1;3mLangChain provides interfaces and integrations for language models and application-specific data. It's designed to support putting prototypes in production, from simple to complex chains. It's part of a rich ecosystem of tools that work together to simplify the entire application lifecycle. However, a more explicit definition is still needed.\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"LangChain explained\"\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://blog.enterprisedna.co/what-is-langchain-a-beginners-guide-with-examples/', 'content': 'The following are some of the key features of LangChain:\\nCustomizable prompts to suit your needs\\nBuilding chain link components for advanced use cases\\nCode customization for developing unique applications\\nModel integration for data augmented generation and accessing high-quality language model application like text-davinci-003\\nFlexible components to mix and match components for specific requirements\\nContext manipulation to set and guide context for improved accuracy and user experience\\nWith LangChain, you can create feature-rich applications that stand out from the crowd, thanks to its advanced customization options.\\n Following are some of the key resources that you can use when working with LangChain:\\nAI Libraries such as OpenAI and Hugging Face for AI models\\nExternal sources such as Notion, Wikipedia, and Google Drive for targeted data\\nLangChain documentation for guides on connecting and chaining components\\nOfficial Documentation\\nGitHub Repository\\nPyPI Package Repository\\nData augmentation to improve context-aware results through external data sources, indexing, and vector representations\\nLastly, engaging with the LangChain community and dedicated support slack channel can be beneficial if you encounter challenges or want to learn from othersâ€™ experiences. Day(s)\\n:\\nHour(s)\\n:\\nMinute(s)\\n:\\nSecond(s)\\nBlog\\nDay(s)\\n:\\nHour(s)\\n:\\nMinute(s)\\n:\\nSecond(s)\\n So, if youâ€™re looking to stay ahead of the curve in the world of NLP, be sure to check out LangChain and see what it can do for you!\\nRelated Posts\\nText to Code Generator: Create Code in any Language\\nAI\\nHave you ever wondered if there was an easier way to write code? Generate Accurate Code in Seconds\\nAI, Data Mentor\\nWelcome to the exhilarating world of AI Code Generators, where imagination meets execution at lightning...\\nJava Code Generator: How to Generate Java Code Quickly\\nAI, Data Mentor, Java\\nIntegrating artificial intelligence (AI) and natural language processing (NLP) has revolutionized how...\\nC++ Code Generator:'}]\u001b[0m\u001b[32;1m\u001b[1;3mLangChain appears to be a tool that allows for the creation of feature-rich applications, with advanced customization options. It includes customizable prompts, chain link components, code customization, model integration, and flexible components for specific requirements. It also allows for context manipulation to improve accuracy and user experiences. It seems to be particularly useful in the area of Natural Language Processing (NLP). However, a more precise definition is still needed.\n",
      "Action: tavily_search_results_json\n",
      "Action Input: \"LangChain product description\"\u001b[0m\u001b[36;1m\u001b[1;3m[{'url': 'https://python.langchain.com/docs/get_started/introduction', 'content': \"Modules\\u200b\\nLangChain provides standard, extendable interfaces and integrations for the following modules:\\nInterface with language models\\nInterface with application-specific data\\nLet models choose which tools to use given high-level directives\\nExamples, ecosystem, and resources\\u200b\\nUse cases\\u200b\\nWalkthroughs and techniques for common end-to-end use cases, like:\\nIntegrations\\u200b\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Together, these products simplify the entire application lifecycle:\\nLangChain Libraries\\u200b\\nThe main value props of the LangChain packages are:\\nOff-the-shelf chains make it easy to get started. API reference\\u200b\\nHead to the reference section for full documentation of all classes and methods in the LangChain and LangChain Experimental Python packages.\\n Developer's guide\\u200b\\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\\n LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest â€œprompt + LLMâ€ chain to the most complex chains.\\n\"}]\u001b[0m\u001b[32;1m\u001b[1;3mBased on the observations and the information gathered so far, it seems that LangChain is a tool that provides standard, extendable interfaces and integrations for interfacing with language models and application-specific data. It simplifies the application lifecycle in the context of language model applications and supports the transition of prototypes into production. It is part of a broader ecosystem of tools and provides a rich set of features for application customization.\n",
      "Final Answer: LangChain is a tool for building language model applications. It provides standardized interfaces and integrations for interfacing with language models and application-specific data, making it easier to build and customize applications. It is designed to simplify the entire application lifecycle and supports the transition from prototypes to production. It is part of a broader ecosystem of tools and offers a range of features for application customization.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is LangChain?',\n",
       " 'output': 'LangChain is a tool for building language model applications. It provides standardized interfaces and integrations for interfacing with language models and application-specific data, making it easier to build and customize applications. It is designed to simplify the entire application lifecycle and supports the transition from prototypes to production. It is part of a broader ecosystem of tools and offers a range of features for application customization.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"what is LangChain?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use chat hhistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/react-chat\")\n",
    "\n",
    "# Construct the ReAct agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Do I need to use a tool? No\n",
      "Final Answer: Your name is Bob.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"what's my name? Only use a tool if needed, otherwise respond with Final Answer\",\n",
       " 'chat_history': 'Human: Hi! My name is Bob\\nAI: Hello Bob! Nice to meet you',\n",
       " 'output': 'Your name is Bob.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"what's my name? Only use a tool if needed, otherwise respond with Final Answer\",\n",
    "        # Notice that chat_history is a string, since this prompt is aimed at LLMs, not chat models\n",
    "        \"chat_history\": \"Human: Hi! My name is Bob\\nAI: Hello Bob! Nice to meet you\",\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
